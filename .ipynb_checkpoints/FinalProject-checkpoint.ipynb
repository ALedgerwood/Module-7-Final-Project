{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f68323-485d-42bc-b20a-1e863da925df",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Alex Ledgerwood\n",
    "\n",
    "Github Repo: https://github.com/ALedgerwood/Module-7-Final-Project\n",
    "Link to Tutorial:https://realpython.com/natural-language-processing-spacy-python/\n",
    "Link to Text to be analyzed:https://readingwise.com/assets/uploads/pdf/The_Hill_We_Climb_Transcript.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee0dcd-9a69-49c8-84a9-527ea6ec090c",
   "metadata": {},
   "source": [
    "### import spacy and define nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146cdae4-3b80-4423-9f7a-fd4b7f54cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x150938efa60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cee52-b123-41fe-8b4e-1524a758f360",
   "metadata": {},
   "source": [
    "### call the .text attribute on each token to get the text contained within that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8903c4fa-2f1b-4364-8e54-180d218ef19d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'spaCy',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
    "type(introduction_doc)\n",
    "\n",
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db86815-d30e-4862-9ac5-0176a290f2ef",
   "metadata": {},
   "source": [
    "### do the same thing but reading from a file, not typing it in\n",
    "\n",
    "I don't know where the introduction.txt file is supposed to be - I am using the exact code from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d7feb0-ca8a-447b-8a14-719e51a7538e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'introduction.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintroduction.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m introduction_doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m([token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m introduction_doc])\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:1266\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'introduction.txt'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "file_name = \"introduction.txt\"\n",
    "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
    "print([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa8f2b-1b66-4134-9f44-e469ccef55bb",
   "metadata": {},
   "source": [
    "### the .sents property is used to extract sentences from the Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e88a2b3-1658-4182-9a1d-0206910efc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python...\n",
      "He is interested in learning...\n"
     ]
    }
   ],
   "source": [
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    ">>> sentences = list(about_doc.sents)\n",
    ">>> len(sentences)\n",
    "2\n",
    ">>> for sentence in sentences:\n",
    "...     print(f\"{sentence[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef10447-734c-453a-8db6-d1157ba0364f",
   "metadata": {},
   "source": [
    "### Creating custom delimeters in sentence detection\n",
    "\n",
    "this uses elipses as a delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7340b72f-580d-48fb-87c9-08366328f1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    ">>> ellipsis_text = (\n",
    "...     \"Gus, can you, ... never mind, I forgot\"\n",
    "...     \" what I was saying. So, do you think\"\n",
    "...     \" we should ...\"\n",
    "... )\n",
    "\n",
    ">>> from spacy.language import Language\n",
    ">>> @Language.component(\"set_custom_boundaries\")\n",
    "... def set_custom_boundaries(doc):\n",
    "...     \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
    "...     for token in doc[:-1]:\n",
    "...         if token.text == \"...\":\n",
    "...             doc[token.i + 1].is_sent_start = True\n",
    "...     return doc\n",
    "...\n",
    "\n",
    ">>> custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    ">>> custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    ">>> custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    ">>> for sentence in custom_ellipsis_sentences:\n",
    "...     print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5310147-5f15-4f76-add0-3ae00df97ee3",
   "metadata": {},
   "source": [
    "### Tokenization - showing that the token’s original index position in the string is still available as an attribute on Token\n",
    "\n",
    "could be useful for in-place word replacement down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a1ee85d-68a4-410b-bade-b183c4ec8da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    "\n",
    ">>> for token in about_doc:\n",
    "...     print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86951c-6869-476e-b08b-6ae5e8a6f3dc",
   "metadata": {},
   "source": [
    "### Other attributes for the token class\n",
    "use f-string formatting to output a table accessing some common attributes from each Token in Doc:\n",
    "\n",
    ".text_with_ws prints the token text along with any trailing space, if present.\n",
    "\n",
    ".is_alpha indicates whether the token consists of alphabetic characters or not.\n",
    "\n",
    ".is_punct indicates whether the token is a punctuation symbol or not.\n",
    "\n",
    ".is_stop indicates whether the token is a stop word or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ec7791-a562-494e-8a4a-b0205fe76fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
      "Gus                   True           False             False\n",
      "Proto                 True           False             False\n",
      "is                    True           False             True\n",
      "a                     True           False             True\n",
      "Python                True           False             False\n",
      "developer             True           False             False\n",
      "currently             True           False             False\n",
      "working               True           False             False\n",
      "for                   True           False             True\n",
      "a                     True           False             True\n",
      "London                True           False             False\n",
      "-                     False          True              False\n",
      "based                 True           False             False\n",
      "Fintech               True           False             False\n",
      "company               True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "Natural               True           False             False\n",
      "Language              True           False             False\n",
      "Processing            True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    ">>> print(\n",
    "...     f\"{'Text with Whitespace':22}\"\n",
    "...     f\"{'Is Alphanumeric?':15}\"\n",
    "...     f\"{'Is Punctuation?':18}\"\n",
    "...     f\"{'Is Stop Word?'}\"\n",
    "... )\n",
    ">>> for token in about_doc:\n",
    "...     print(\n",
    "...         f\"{str(token.text_with_ws):22}\"\n",
    "...         f\"{str(token.is_alpha):15}\"\n",
    "...         f\"{str(token.is_punct):18}\"\n",
    "...         f\"{str(token.is_stop)}\"\n",
    "...     )\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72658f84-c377-4689-9e52-e94f2713653c",
   "metadata": {},
   "source": [
    "### custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67bb31a-b169-41a7-a4c7-941a5ad0d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London@based', 'Fintech', 'company', '.', 'He']\n"
     ]
    }
   ],
   "source": [
    ">>> custom_about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London@based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    "\n",
    ">>> print([token.text for token in nlp(custom_about_text)[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e9ff8-6ac4-4ebc-82a7-4fe4fe028bd9",
   "metadata": {},
   "source": [
    "### To include the @ symbol as a custom infix, you need to build your own Tokenizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd08a0d-6791-49ba-b691-72d8a894a754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    ">>> import re\n",
    ">>> from spacy.tokenizer import Tokenizer\n",
    "\n",
    ">>> custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> prefix_re = spacy.util.compile_prefix_regex(\n",
    "...     custom_nlp.Defaults.prefixes\n",
    "... )\n",
    ">>> suffix_re = spacy.util.compile_suffix_regex(\n",
    "...     custom_nlp.Defaults.suffixes\n",
    "... )\n",
    "\n",
    ">>> custom_infixes = [r\"@\"]\n",
    "\n",
    ">>> infix_re = spacy.util.compile_infix_regex(\n",
    "...     list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    "... )\n",
    "\n",
    ">>> custom_nlp.tokenizer = Tokenizer(\n",
    "...     nlp.vocab,\n",
    "...     prefix_search=prefix_re.search,\n",
    "...     suffix_search=suffix_re.search,\n",
    "...     infix_finditer=infix_re.finditer,\n",
    "...     token_match=None,\n",
    "... )\n",
    "\n",
    ">>> custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "\n",
    ">>> print([token.text for token in custom_tokenizer_about_doc[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4a09a-f303-45ae-83de-ed04527616c5",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Stop words are typically defined as the most common words in a language.\n",
    "\n",
    "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b68806-b0f9-4d18-b19a-085832895f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did\n",
      "they\n",
      "thru\n",
      "yet\n",
      "’m\n",
      "would\n",
      "whole\n",
      "hereafter\n",
      "say\n",
      "under\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    ">>> len(spacy_stopwords)\n",
    "326\n",
    ">>> for stop_word in list(spacy_stopwords)[:10]:\n",
    "...     print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d14ca9-db2d-455e-86cd-a1f426246d90",
   "metadata": {},
   "source": [
    "### .is_stop attribute\n",
    "You don’t need to access this list directly, though. You can REMOVE STOP WORDS from the input text by making use of the .is_stop attribute of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac00cb07-822f-4a7c-aa91-8bd2475b6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    ">>> custom_about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_doc = nlp(custom_about_text)\n",
    ">>> print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8ac64-f5f0-484e-87c3-7551a8fa5315",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "a root word, is called a lemma.\n",
    "\n",
    "reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc87a1d5-9487-4e0d-9b88-28d81d575a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is : be\n",
      "                  He : he\n",
      "               keeps : keep\n",
      "          organizing : organize\n",
      "             meetups : meetup\n",
      "               talks : talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> conference_help_text = (\n",
    "...     \"Gus is helping organize a developer\"\n",
    "...     \" conference on Applications of Natural Language\"\n",
    "...     \" Processing. He keeps organizing local Python meetups\"\n",
    "...     \" and several internal talks at his workplace.\"\n",
    "... )\n",
    ">>> conference_help_doc = nlp(conference_help_text)\n",
    ">>> for token in conference_help_doc:\n",
    "...     if str(token) != str(token.lemma_):\n",
    "...         print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bfdc7-a3b8-4ae0-92fa-cbfcc48bf70c",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "once you've lemmatized (AND removed stop words), you can perform statitstical analysis on text\n",
    "\n",
    "the following code tells the most common words, so you can assume they are what the text is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12746896-600e-4954-8d37-e0299e4e2a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> from collections import Counter\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> complete_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech company. He is\"\n",
    "...     \" interested in learning Natural Language Processing.\"\n",
    "...     \" There is a developer conference happening on 21 July\"\n",
    "...     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "...     ' Language Processing\". There is a helpline number'\n",
    "...     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "...     \" He keeps organizing local Python meetups and several\"\n",
    "...     \" internal talks at his workplace. Gus is also presenting\"\n",
    "...     ' a talk. The talk will introduce the reader about \"Use'\n",
    "...     ' cases of Natural Language Processing in Fintech\".'\n",
    "...     \" Apart from his work, he is very passionate about music.\"\n",
    "...     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "...     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "...     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "...     \" of London and has world-class piano instructors.\"\n",
    "... )\n",
    ">>> complete_doc = nlp(complete_text)\n",
    "\n",
    ">>> words = [\n",
    "...     token.text\n",
    "...     for token in complete_doc\n",
    "...     if not token.is_stop and not token.is_punct\n",
    "... ]\n",
    "\n",
    ">>> print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfe823-2454-4add-9c10-f320339596a4",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5620d976-0f7e-42a5-af17-5d908775fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Proto\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: Python\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: developer\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: currently\n",
      "=====\n",
      "TAG: RB         POS: ADV\n",
      "EXPLANATION: adverb\n",
      "\n",
      "TOKEN: working\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: London\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: -\n",
      "=====\n",
      "TAG: HYPH       POS: PUNCT\n",
      "EXPLANATION: punctuation mark, hyphen\n",
      "\n",
      "TOKEN: based\n",
      "=====\n",
      "TAG: VBN        POS: VERB\n",
      "EXPLANATION: verb, past participle\n",
      "\n",
      "TOKEN: Fintech\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: company\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "=====\n",
      "TAG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: interested\n",
      "=====\n",
      "TAG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: in\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: Natural\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Language\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Processing\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    ">>> for token in about_doc:\n",
    "...     print(\n",
    "...         f\"\"\"\n",
    "... TOKEN: {str(token)}\n",
    "... =====\n",
    "... TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "... EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "...     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebb28b-7fdd-43b0-a02b-8a0f46b8d8b7",
   "metadata": {},
   "source": [
    "### Pullout words by part of speech/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8f4486-6690-4b62-a0e1-8bde38d09187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[developer, company]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> nouns = []\n",
    ">>> adjectives = []\n",
    ">>> for token in about_doc:\n",
    "...     if token.pos_ == \"NOUN\":\n",
    "...         nouns.append(token)\n",
    "...     if token.pos_ == \"ADJ\":\n",
    "...         adjectives.append(token)\n",
    "...\n",
    "\n",
    ">>> nouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e72636-fb53-4d71-9135-99151cd937d1",
   "metadata": {},
   "source": [
    "### Visualization using spaCy's builtin called displaCy\n",
    "\n",
    "each token is assigned a POS tag written just below the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ced78fe-5718-4b1b-bacb-4a5ba4e11148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19139\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c9d44798ecea43b58f73d84948d2ad45-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c9d44798ecea43b58f73d84948d2ad45-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c9d44798ecea43b58f73d84948d2ad45-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> from spacy import displacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    ">>> about_interest_text = (\n",
    "...     \"He is interested in learning Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_interest_doc = nlp(about_interest_text)\n",
    ">>> displacy.serve(about_interest_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e885e-b35c-4426-bdaf-0042864704d1",
   "metadata": {},
   "source": [
    "### Creating Preprocessing Functions\n",
    "To bring your text into a format ideal for analysis, you can write preprocessing functions to encapsulate your cleaning process.\n",
    "\n",
    "Note that complete_filtered_tokens doesn’t contain any stop words or punctuation symbols, and it consists purely of lemmatized lowercase tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1c576f-b1bd-4766-8721-a10c6f6d86ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gus',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'developer',\n",
       " 'currently',\n",
       " 'work',\n",
       " 'london',\n",
       " 'base',\n",
       " 'fintech',\n",
       " 'company',\n",
       " 'interested',\n",
       " 'learn',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'developer',\n",
       " 'conference',\n",
       " 'happen',\n",
       " '21',\n",
       " 'july',\n",
       " '2019',\n",
       " 'london',\n",
       " 'title',\n",
       " 'application',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'helpline',\n",
       " 'number',\n",
       " 'available',\n",
       " '+44',\n",
       " '1234567891',\n",
       " 'gus',\n",
       " 'helping',\n",
       " 'organize',\n",
       " 'keep',\n",
       " 'organize',\n",
       " 'local',\n",
       " 'python',\n",
       " 'meetup',\n",
       " 'internal',\n",
       " 'talk',\n",
       " 'workplace',\n",
       " 'gus',\n",
       " 'present',\n",
       " 'talk',\n",
       " 'talk',\n",
       " 'introduce',\n",
       " 'reader',\n",
       " 'use',\n",
       " 'case',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'fintech',\n",
       " 'apart',\n",
       " 'work',\n",
       " 'passionate',\n",
       " 'music',\n",
       " 'gus',\n",
       " 'learn',\n",
       " 'play',\n",
       " 'piano',\n",
       " 'enrol',\n",
       " 'weekend',\n",
       " 'batch',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'situate',\n",
       " 'mayfair',\n",
       " 'city',\n",
       " 'london',\n",
       " 'world',\n",
       " 'class',\n",
       " 'piano',\n",
       " 'instructor']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> complete_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech company. He is\"\n",
    "...     \" interested in learning Natural Language Processing.\"\n",
    "...     \" There is a developer conference happening on 21 July\"\n",
    "...     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "...     ' Language Processing\". There is a helpline number'\n",
    "...     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "...     \" He keeps organizing local Python meetups and several\"\n",
    "...     \" internal talks at his workplace. Gus is also presenting\"\n",
    "...     ' a talk. The talk will introduce the reader about \"Use'\n",
    "...     ' cases of Natural Language Processing in Fintech\".'\n",
    "...     \" Apart from his work, he is very passionate about music.\"\n",
    "...     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "...     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "...     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "...     \" of London and has world-class piano instructors.\"\n",
    "... )\n",
    ">>> complete_doc = nlp(complete_text)\n",
    ">>> def is_token_allowed(token):\n",
    "...     return bool(\n",
    "...         token\n",
    "...         and str(token).strip()\n",
    "...         and not token.is_stop\n",
    "...         and not token.is_punct\n",
    "...     )\n",
    "...\n",
    ">>> def preprocess_token(token):\n",
    "...     return token.lemma_.strip().lower()\n",
    "...\n",
    ">>> complete_filtered_tokens = [\n",
    "...     preprocess_token(token)\n",
    "...     for token in complete_doc\n",
    "...     if is_token_allowed(token)\n",
    "... ]\n",
    "\n",
    ">>> complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d5cbe-3c09-419f-97eb-90d6dde879fd",
   "metadata": {},
   "source": [
    "### Rule-cased matching using spaCy\n",
    "For example, with rule-based matching, you can extract a first name and a last name, which are always proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedec966-7fbd-457b-b170-27f8b8e1051e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gus Proto'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_text = (\n",
    "...     \"Gus Proto is a Python developer currently\"\n",
    "...     \" working for a London-based Fintech\"\n",
    "...     \" company. He is interested in learning\"\n",
    "...     \" Natural Language Processing.\"\n",
    "... )\n",
    ">>> about_doc = nlp(about_text)\n",
    "\n",
    ">>> from spacy.matcher import Matcher\n",
    ">>> matcher = Matcher(nlp.vocab)\n",
    "\n",
    ">>> def extract_full_name(nlp_doc):\n",
    "...     pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "...     matcher.add(\"FULL_NAME\", [pattern])\n",
    "...     matches = matcher(nlp_doc)\n",
    "...     for _, start, end in matches:\n",
    "...         span = nlp_doc[start:end]\n",
    "...         yield span.text\n",
    "...\n",
    "\n",
    ">>> next(extract_full_name(about_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f41e97-00f6-4f51-b4e3-95f8cc56874a",
   "metadata": {},
   "source": [
    "### Dependency Parsing Using spaCy\n",
    "\n",
    "Extracting the dependency graph of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. \n",
    "\n",
    "The dependencies can be mapped in a directed graph representation where:\n",
    "\n",
    "Words are the nodes.\n",
    "\n",
    "Grammatical relationships are the edges.\n",
    "\n",
    "In this example, the sentence contains three relationships:\n",
    "\n",
    "nsubj is the subject of the word, and its headword is a verb.\n",
    "\n",
    "aux is an auxiliary word, and its headword is a verb.\n",
    "\n",
    "dobj is the direct object of the verb, and its headword is also a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab4c56f-d53a-4af8-af7b-ac6bfda914d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "=====\n",
      "token.tag_ = 'NNP'\n",
      "token.head.text = 'learning'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'learning'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'learning'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: piano\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'learning'\n",
      "token.dep_ = 'dobj'\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    "from spacy import displacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> piano_text = \"Gus is learning piano\"\n",
    ">>> piano_doc = nlp(piano_text)\n",
    ">>> for token in piano_doc:\n",
    "...     print(\n",
    "...         f\"\"\"\n",
    "... TOKEN: {token.text}\n",
    "... =====\n",
    "... {token.tag_ = }\n",
    "... {token.head.text = }\n",
    "... {token.dep_ = }\"\"\"\n",
    "...     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a177dc-cd8d-4810-9ef3-3a36dddf04bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19139\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4726528b69774c24b83b0cd2116a24a8-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Gus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">piano</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4726528b69774c24b83b0cd2116a24a8-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4726528b69774c24b83b0cd2116a24a8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4726528b69774c24b83b0cd2116a24a8-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4726528b69774c24b83b0cd2116a24a8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4726528b69774c24b83b0cd2116a24a8-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4726528b69774c24b83b0cd2116a24a8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    ">>> displacy.serve(piano_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814782f7-50f0-49a6-976f-ab535cce1ffa",
   "metadata": {},
   "source": [
    "### Tree and subtree navigation\n",
    "spaCy provides attributes like .children, .lefts, .rights, and .subtree to make navigating the parse tree easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efef9d5-bef6-43eb-9c9c-e0c6d01cf959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Python', 'working']\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> one_line_about_text = (\n",
    "...     \"Gus Proto is a Python developer\"\n",
    "...     \" currently working for a London-based Fintech company\"\n",
    "... )\n",
    ">>> one_line_about_doc = nlp(one_line_about_text)\n",
    "\n",
    ">>> # Extract children of `developer`\n",
    ">>> print([token.text for token in one_line_about_doc[5].children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaaa616-a731-4a6b-af1c-e8a41fc8d5da",
   "metadata": {},
   "source": [
    "### Shallow Parsing/Chunking\n",
    "\n",
    "Noun Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a4917d-a0e2-491b-98cd-dfcc98a14169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a developer conference\n",
      "21 July\n",
      "London\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    ">>> conference_text = (\n",
    "...     \"There is a developer conference happening on 21 July 2019 in London.\"\n",
    "... )\n",
    ">>> conference_doc = nlp(conference_text)\n",
    "\n",
    ">>> # Extract Noun Phrases\n",
    ">>> for chunk in conference_doc.noun_chunks:\n",
    "...     print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6493d71-ee8e-4d92-ae1f-3a9407c01fb8",
   "metadata": {},
   "source": [
    "### Name-Entity Recognition\n",
    "locating named entities in unstructured text and then classifying them into predefined categories, such as person names, organizations, locations, etc\n",
    "\n",
    "You can use NER to learn more about the meaning of your text.\n",
    "\n",
    "spaCy has the property .ents on Doc objects. You can use it to extract named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658d4c4c-8f04-4fec-ab69-278679e8cef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ent.text = 'Great Piano Academy'\n",
      "ent.start_char = 0\n",
      "ent.end_char = 19\n",
      "ent.label_ = 'ORG'\n",
      "spacy.explain('ORG') = Companies, agencies, institutions, etc.\n",
      "\n",
      "ent.text = 'Mayfair'\n",
      "ent.start_char = 35\n",
      "ent.end_char = 42\n",
      "ent.label_ = 'GPE'\n",
      "spacy.explain('GPE') = Countries, cities, states\n",
      "\n",
      "ent.text = 'the City of London'\n",
      "ent.start_char = 46\n",
      "ent.end_char = 64\n",
      "ent.label_ = 'GPE'\n",
      "spacy.explain('GPE') = Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    "from spacy import displacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    ">>> piano_class_text = (\n",
    "...     \"Great Piano Academy is situated\"\n",
    "...     \" in Mayfair or the City of London and has\"\n",
    "...     \" world-class piano instructors.\"\n",
    "... )\n",
    ">>> piano_class_doc = nlp(piano_class_text)\n",
    "\n",
    ">>> for ent in piano_class_doc.ents:\n",
    "...     print(\n",
    "...         f\"\"\"\n",
    "... {ent.text = }\n",
    "... {ent.start_char = }\n",
    "... {ent.end_char = }\n",
    "... {ent.label_ = }\n",
    "... spacy.explain('{ent.label_}') = {spacy.explain(ent.label_)}\"\"\"\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a453926-2fd1-426f-8fc9-0c5a98bcc858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19139\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Piano Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_class_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c794de-6ab0-4b97-a49b-391a994e47f2",
   "metadata": {},
   "source": [
    "### Using NER to Redact names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4078e27c-5ca4-4e2d-a956-966a5af5cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5 people surveyed, [REDACTED] , [REDACTED] and [REDACTED] like apples. [REDACTED] and [REDACTED] like oranges.\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> survey_text = (\n",
    "...     \"Out of 5 people surveyed, James Robert,\"\n",
    "...     \" Julie Fuller and Benjamin Brooks like\"\n",
    "...     \" apples. Kelly Cox and Matthew Evans\"\n",
    "...     \" like oranges.\"\n",
    "... )\n",
    "\n",
    "\n",
    ">>> def replace_person_names(token):\n",
    "...     if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "...         return \"[REDACTED] \"\n",
    "...     return token.text_with_ws\n",
    "...\n",
    "\n",
    ">>> def redact_names(nlp_doc):\n",
    "...     with nlp_doc.retokenize() as retokenizer:\n",
    "...         for ent in nlp_doc.ents:\n",
    "...             retokenizer.merge(ent)\n",
    "...     tokens = map(replace_person_names, nlp_doc)\n",
    "...     return \"\".join(tokens)\n",
    "...\n",
    "\n",
    ">>> survey_doc = nlp(survey_text)\n",
    ">>> print(redact_names(survey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3983c2-7099-409f-9866-1b96d1aa2578",
   "metadata": {},
   "source": [
    "### That's it for the tutorial. Now I'm going to try to get python to read from an online file and do some of the same nlp analysis on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a6b95e-8dc3-4353-a8ad-02924a28056d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://readingwise.com/assets/uploads/pdf/The_Hill_We_Climb_Transcript.pdft\"\n",
    "response = requests.get(url)\n",
    "text_data = response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24c4c47-95d2-4351-acab-6b36ecbbbbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load English language model\n",
    "\n",
    "doc = nlp(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4669e06-1cca-4ed9-840e-40c235d5b578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "The Hill We Climb  Transcript from Amanda Gorman’s reading at President Joe Biden’s inauguration, 20th January 2021  When day comes we ask ourselves, where can we find light in this never-ending shade? The loss we carry, a sea we must wade We've braved the belly of the beast We've learned that quiet isn't always peace And the norms and notions of what just is Isn’t always just-ice And yet the dawn is ours we knew it Somehow we do it Somehow we've weathered and witnessed a nation that isn’t  but simply unfinished We the successors of a country and a time Where a skinny Black girl descended from slaves and raised by a single mother can dream of becoming president only to find herself reciting for one And yes we are far from polished far from pristine but that doesn’t mean we are striving to form a union that is perfect We are striving to forge a union with purpose To compose a country committed to all cultures, colours, characters and conditions of man And so we lift our gazes not to what stands between us but what stands before us We close the divide because we know, to put our future first, we must first put our differences aside We lay down our arms so we can reach out our arms to one another We seek harm to none and harmony for all Let the globe, if nothing else, say this is true: That even as we grieved, we grew That even as we hurt, we hoped That even as we tired, we tried That we’ll forever be tied together, victorious Not because we will never again know defeat but because we will never again sow division Scripture tells us to envision that everyone shall sit under their own vine and fig tree And no one shall make them afraid If we’re to live up to our own time Then victory won’t lie in the blade But in all the bridges we’ve made That is the promise to glade The hill we climb If only we dare It's because being American is more than a pride we inherit, it’s the past we step into and how we repair it We’ve seen a force that would shatter our nation  \n",
      " \n",
      "rather than share it Would destroy our country if it meant delaying democracy And this effort very nearly succeeded But while democracy can be periodically delayed it can never be permanently defeated In this truth in this faith we trust For while we have our eyes on the future history has its eyes on us This is the era of just redemption We feared at its inception We did not feel prepared to be the heirs of such a terrifying hour but within it we found the power to author a new chapter To offer hope and laughter to ourselves So while once we asked, how could we possibly prevail over catastrophe? Now we assert How could catastrophe possibly prevail over us? We will not march back to what was but move to what shall be A country that is bruised but whole, benevolent but bold, fierce and free We will not be turned around or interrupted by intimidation because we know our inaction and inertia will be the inheritance of the next generation Our blunders become their burdens But one thing is certain: If we merge mercy with might, and might with right, then love becomes our legacy and change our children’s birthright So let us leave behind a country better than the one we were left with Every breath from my bronze-pounded chest, we will raise this wounded world into a wondrous one We will rise from the gold-limbed hills of the west, we will rise from the windswept northeast where our forefathers first realized revolution We will rise from the lake-rimmed cities of the midwestern states, we will rise from the sunbaked south We will rebuild, reconcile and recover and every known nook of our nation and every corner called our country, our people diverse and beautiful will emerge, battered and beautiful When day comes we step out of the shade, aflame and unafraid The new dawn blooms as we free it For there is always light, if only we’re brave enough to see it If only we’re brave enough to be it  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "\n",
    "# Fetch the PDF file\n",
    "pdf_url = \"https://readingwise.com/assets/uploads/pdf/The_Hill_We_Climb_Transcript.pdf\"\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "# Wrap the content in a BytesIO object\n",
    "pdf_bytes = BytesIO(response.content)\n",
    "\n",
    "# Create a PDF reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_bytes)\n",
    "\n",
    "# Initialize an empty string to store extracted text\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Iterate through each page and extract text\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    page = pdf_reader.pages[page_num]\n",
    "    extracted_text += page.extract_text()\n",
    "\n",
    "# Print or process the extracted text as needed\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab22089-acf7-4d3d-b7ea-eeb4dfeca757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "them\n",
      "as\n",
      "else\n",
      "none\n",
      "some\n",
      "give\n",
      "out\n",
      "almost\n",
      "how\n",
      "rather\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    ">>> len(spacy_stopwords)\n",
    "326\n",
    ">>> for stop_word in list(spacy_stopwords)[:10]:\n",
    "...     print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef0261d6-387f-4535-bbb7-383326665dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \n",
      " \n",
      ", Hill, Climb,  , Transcript, Amanda, Gorman, reading, President, Joe, Biden, inauguration, ,, 20th, January, 2021,  , day, comes, ask, ,, find, light, -, ending, shade, ?, loss, carry, ,, sea, wade, braved, belly, beast, learned, quiet, peace, norms, notions, -, ice, dawn, knew, weathered, witnessed, nation,  , simply, unfinished, successors, country, time, skinny, Black, girl, descended, slaves, raised, single, mother, dream, president, find, reciting, yes, far, polished, far, pristine, mean, striving, form, union, perfect, striving, forge, union, purpose, compose, country, committed, cultures, ,, colours, ,, characters, conditions, man, lift, gazes, stands, stands, close, divide, know, ,, future, ,, differences, aside, lay, arms, reach, arms, seek, harm, harmony, Let, globe, ,, ,, true, :, grieved, ,, grew, hurt, ,, hoped, tired, ,, tried, forever, tied, ,, victorious, know, defeat, sow, division, Scripture, tells, envision, shall, sit, vine, fig, tree, shall, afraid, live, time, victory, wo, lie, blade, bridges, promise, glade, hill, climb, dare, American, pride, inherit, ,, past, step, repair, seen, force, shatter, nation,  \n",
      " \n",
      ", share, destroy, country, meant, delaying, democracy, effort, nearly, succeeded, democracy, periodically, delayed, permanently, defeated, truth, faith, trust, eyes, future, history, eyes, era, redemption, feared, inception, feel, prepared, heirs, terrifying, hour, found, power, author, new, chapter, offer, hope, laughter, asked, ,, possibly, prevail, catastrophe, ?, assert, catastrophe, possibly, prevail, ?, march, shall, country, bruised, ,, benevolent, bold, ,, fierce, free, turned, interrupted, intimidation, know, inaction, inertia, inheritance, generation, blunders, burdens, thing, certain, :, merge, mercy, ,, right, ,, love, legacy, change, children, birthright, let, leave, country, better, left, breath, bronze, -, pounded, chest, ,, raise, wounded, world, wondrous, rise, gold, -, limbed, hills, west, ,, rise, windswept, northeast, forefathers, realized, revolution, rise, lake, -, rimmed, cities, midwestern, states, ,, rise, sunbaked, south, rebuild, ,, reconcile, recover, known, nook, nation, corner, called, country, ,, people, diverse, beautiful, emerge, ,, battered, beautiful, day, comes, step, shade, ,, aflame, unafraid, new, dawn, blooms, free, light, ,, brave, brave,  ]\n"
     ]
    }
   ],
   "source": [
    ">>> import re\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> about_doc = nlp(extracted_text)\n",
    ">>> cleaned_text = re.sub(r'\\s+', ' ', extracted_text).strip()\n",
    ">>> print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b1b945d-93aa-45bb-bfdc-2d695b773946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 The : the\n",
      "                  We : we\n",
      "                When : when\n",
      "               comes : come\n",
      "              ending : end\n",
      "                 The : the\n",
      "                  We : we\n",
      "              braved : brave\n",
      "                  We : we\n",
      "             learned : learn\n",
      "                  is : be\n",
      "                 n't : not\n",
      "                 And : and\n",
      "               norms : norm\n",
      "             notions : notion\n",
      "                  is : be\n",
      "                  Is : be\n",
      "                 n’t : not\n",
      "                 And : and\n",
      "                  is : be\n",
      "                knew : know\n",
      "             Somehow : somehow\n",
      "             Somehow : somehow\n",
      "           weathered : weather\n",
      "           witnessed : witness\n",
      "                  is : be\n",
      "                 n’t : not\n",
      "                  We : we\n",
      "          successors : successor\n",
      "               Where : where\n",
      "               Black : black\n",
      "           descended : descend\n",
      "              slaves : slave\n",
      "              raised : raise\n",
      "            becoming : become\n",
      "            reciting : recite\n",
      "                 And : and\n",
      "                 are : be\n",
      "                does : do\n",
      "                 n’t : not\n",
      "                 are : be\n",
      "            striving : strive\n",
      "                  is : be\n",
      "                  We : we\n",
      "                 are : be\n",
      "            striving : strive\n",
      "                  To : to\n",
      "           committed : commit\n",
      "            cultures : culture\n",
      "             colours : colour\n",
      "          characters : character\n",
      "          conditions : condition\n",
      "                 And : and\n",
      "               gazes : gaze\n",
      "              stands : stand\n",
      "                  us : we\n",
      "              stands : stand\n",
      "                  us : we\n",
      "                  We : we\n",
      "         differences : difference\n",
      "                  We : we\n",
      "                 lay : lie\n",
      "                arms : arm\n",
      "                arms : arm\n",
      "                  We : we\n",
      "                 Let : let\n",
      "                  is : be\n",
      "                That : that\n",
      "             grieved : grieve\n",
      "                grew : grow\n",
      "                That : that\n",
      "               hoped : hope\n",
      "                That : that\n",
      "               tired : tire\n",
      "               tried : try\n",
      "                That : that\n",
      "                tied : tie\n",
      "                 Not : not\n",
      "               tells : tell\n",
      "                  us : we\n",
      "                 And : and\n",
      "                them : they\n",
      "                  If : if\n",
      "                Then : then\n",
      "                  wo : will\n",
      "                 n’t : not\n",
      "                 But : but\n",
      "             bridges : bridge\n",
      "                made : make\n",
      "                That : that\n",
      "                  is : be\n",
      "                 The : the\n",
      "                  If : if\n",
      "                  It : it\n",
      "                  's : be\n",
      "               being : be\n",
      "            American : american\n",
      "                  is : be\n",
      "                  ’s : ’\n",
      "                  We : we\n",
      "                seen : see\n",
      "               Would : would\n",
      "               meant : mean\n",
      "            delaying : delay\n",
      "                 And : and\n",
      "           succeeded : succeed\n",
      "                 But : but\n",
      "             delayed : delay\n",
      "            defeated : defeat\n",
      "                  In : in\n",
      "                 For : for\n",
      "                eyes : eye\n",
      "                 has : have\n",
      "                eyes : eye\n",
      "                  us : we\n",
      "                This : this\n",
      "                  is : be\n",
      "                  We : we\n",
      "              feared : fear\n",
      "                  We : we\n",
      "                 did : do\n",
      "               heirs : heir\n",
      "               found : find\n",
      "                  To : to\n",
      "                  So : so\n",
      "               asked : ask\n",
      "                 Now : now\n",
      "                 How : how\n",
      "                  us : we\n",
      "                  We : we\n",
      "                 was : be\n",
      "                   A : a\n",
      "                  is : be\n",
      "             bruised : bruise\n",
      "                  We : we\n",
      "              turned : turn\n",
      "         interrupted : interrupt\n",
      "                 Our : our\n",
      "            blunders : blunder\n",
      "             burdens : burden\n",
      "                 But : but\n",
      "                  is : be\n",
      "                  If : if\n",
      "             becomes : become\n",
      "            children : child\n",
      "                  So : so\n",
      "                  us : we\n",
      "              better : well\n",
      "                were : be\n",
      "                left : leave\n",
      "               Every : every\n",
      "             pounded : pound\n",
      "             wounded : wound\n",
      "                  We : we\n",
      "              limbed : limbe\n",
      "               hills : hill\n",
      "         forefathers : forefather\n",
      "            realized : realize\n",
      "                  We : we\n",
      "              rimmed : rim\n",
      "              cities : city\n",
      "              states : state\n",
      "                  We : we\n",
      "               known : know\n",
      "              called : call\n",
      "                When : when\n",
      "               comes : come\n",
      "                 The : the\n",
      "              blooms : bloom\n",
      "                 For : for\n",
      "                  is : be\n",
      "                  If : if\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    ">>> extracted_text_lemma = nlp(extracted_text)\n",
    ">>> for token in extracted_text_lemma:\n",
    "...     if str(token) != str(token.lemma_):\n",
    "...         print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6fd36a8-9d2e-4118-8b7c-5360dd5f5823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('country', 6), (' ', 4), ('rise', 4), ('nation', 3), ('know', 3)]\n"
     ]
    }
   ],
   "source": [
    ">>> import spacy\n",
    ">>> from collections import Counter\n",
    ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    ">>> complete_doc = nlp(extracted_text_lemma)\n",
    "\n",
    ">>> words = [\n",
    "...     token.text\n",
    "...     for token in complete_doc\n",
    "...     if not token.is_stop and not token.is_punct\n",
    "... ]\n",
    "\n",
    ">>> print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254b4be-d269-4219-9b6b-7332777f7273",
   "metadata": {
    "tags": []
   },
   "source": [
    "### this is looking great, but I see a blank space is showing up. I want to include that in the stop words or get rid of it somehow. I'll ask chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8dab89-dd07-4b7c-93a9-dc2cf3cbcecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('country', 6), ('rise', 4), ('nation', 3), ('know', 3), ('shall', 3)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with spaCy\n",
    "complete_doc = nlp(extracted_text_lemma)\n",
    "\n",
    "# Create a list of non-stop, non-punctuation words\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# Count word occurrences\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Print most common words and their counts\n",
    "print(word_count.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750a099-1143-415a-959f-7b42e2301c60",
   "metadata": {},
   "source": [
    "### IT WORKED!!\n",
    "\n",
    "This analysis really does give a good idea about the topic of the poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7a719-316b-40ac-b549-5746a983fbe6",
   "metadata": {},
   "source": [
    "### Now let's try POS tagging to grab all the adjectives\n",
    "\n",
    "Wow, that's powerful!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0405bd15-9c34-474a-8255-76732cc10c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20th,\n",
       " quiet,\n",
       " ours,\n",
       " unfinished,\n",
       " skinny,\n",
       " Black,\n",
       " single,\n",
       " polished,\n",
       " pristine,\n",
       " perfect,\n",
       " true,\n",
       " victorious,\n",
       " own,\n",
       " afraid,\n",
       " own,\n",
       " American,\n",
       " more,\n",
       " future,\n",
       " prepared,\n",
       " terrifying,\n",
       " new,\n",
       " whole,\n",
       " benevolent,\n",
       " bold,\n",
       " fierce,\n",
       " free,\n",
       " next,\n",
       " certain,\n",
       " right,\n",
       " wondrous,\n",
       " windswept,\n",
       " midwestern,\n",
       " sunbaked,\n",
       " diverse,\n",
       " beautiful,\n",
       " battered,\n",
       " beautiful,\n",
       " aflame,\n",
       " new,\n",
       " light,\n",
       " brave,\n",
       " brave]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> adjectives = []\n",
    ">>> for token in about_doc:\n",
    "...     if token.pos_ == \"ADJ\":\n",
    "...         adjectives.append(token)\n",
    "...\n",
    "\n",
    ">>> adjectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8469f-dc58-41d3-be41-b4cd7ad708d5",
   "metadata": {},
   "source": [
    "### I'm amazed I was able make nlp work on a unique text from an online source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282de0c2-c4b0-4f8f-b961-08d5155017da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
